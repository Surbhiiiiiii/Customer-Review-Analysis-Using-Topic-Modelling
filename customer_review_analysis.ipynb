{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0H9zhad29j1ruZrDcATCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surbhiiiiiii/Customer-Review-Analysis-Using-Topic-Modelling/blob/main/customer_review_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4Mh_Y_jbqFQ",
        "outputId": "2e204e73-4cdd-4ca4-983f-f90d0dd05845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: Topic related to: service, food, bad, place, slow\n",
            "Topic 1: Topic related to: like, place, time, got, minute\n",
            "Topic 2: Topic related to: back, probably, food, waited, soon\n",
            "Topic 3: Topic related to: back, place, time, go, would\n",
            "Topic 4: Topic related to: food, good, ever, way, never\n",
            "\n",
            "Extracted Topics and AI Suggestions:\n",
            "\n",
            "\n",
            "ğŸ”¹ **Topic 0:** Topic related to: service, food, bad, place, slow\n",
            "ğŸ’¡ **AI Suggestion:**  1. Top 3 problems customers face:\n",
            "   - Poor service: Customers often complain about slow service, lack of staff attention, and unprofessional behavior from employees.\n",
            "   - Inadequate food quality: Complaints about the taste, freshness, and presentation of food are common.\n",
            "   - Unclean or unpleasant environment: Customers report issues with the cleanliness of facilities, uncomfortable seating, and poor ambiance.\n",
            "\n",
            "2. Specific changes the company should make:\n",
            "   - Improve customer service by training staff to be more attentive, responsive, and friendly.\n",
            "   - Regularly inspect and improve food quality by updating menus, sourcing fresh ingredients, and maintaining proper food handling procedures.\n",
            "   - Increase cleaning schedules and upgrade facilities to ensure a more pleasant dining experience for customers.\n",
            "\n",
            "3. How the company can prevent these issues permanently:\n",
            "   - Implement regular staff training programs to maintain high service standards.\n",
            "   - Conduct regular food quality checks and gather customer feedback to continuously improve menus and sourcing.\n",
            "   - Establish a cleaning schedule and incorporate customer feedback into facility upgrades to ensure a consistent, positive dining experience.\n",
            "\n",
            "\n",
            "ğŸ”¹ **Topic 1:** Topic related to: like, place, time, got, minute\n",
            "ğŸ’¡ **AI Suggestion:**  ğŸ“Œ **Top 3 customer problems:**\n",
            "\n",
            "1. *Like* many other businesses, the company faces issues with long waiting times, leading to customer dissatisfaction.\n",
            "2. The *location or place* of certain stores or service centers may not be convenient for some customers, causing difficulties in accessing services.\n",
            "3. Occasionally, customers *get* poor quality products or services, which results in a negative customer experience.\n",
            "\n",
            "ğŸ“Œ **Specific changes to make:**\n",
            "\n",
            "1. Implement efficient scheduling systems and consider hiring more staff to reduce waiting times and provide prompt service.\n",
            "2. Evaluate current store and service center locations, exploring options to expand or relocate to increase accessibility for customers.\n",
            "3. Establish strict quality control measures and provide regular employee training to ensure consistent product and service quality.\n",
            "\n",
            "ğŸ“Œ **Preventing issues permanently:**\n",
            "\n",
            "1. Monitor and analyze customer wait times regularly and invest in technology to streamline processes, reducing customer frustration.\n",
            "2. Regularly collect customer feedback on location and accessibility, incorporating insights into business decisions for future expansion or relocation.\n",
            "3. Adopt a zero-tolerance policy for poor quality, with consistent inspections, employee accountability and reward systems in place to uphold high standards.\n",
            "\n",
            "\n",
            "ğŸ”¹ **Topic 2:** Topic related to: back, probably, food, waited, soon\n",
            "ğŸ’¡ **AI Suggestion:**  Based on the topic provided, the top 3 problems customers might face are:\n",
            "\n",
            "1. Backordered or out-of-stock food items\n",
            "2. Long waiting times for food delivery or pick-up\n",
            "3. Inaccurate or poor quality of food\n",
            "\n",
            "To address these issues, the company should consider the following specific changes:\n",
            "\n",
            "1. Improve inventory management to minimize backordered or out-of-stock items\n",
            "2. Optimize food delivery/pick-up processes to reduce waiting times\n",
            "3. Implement rigorous quality control measures to ensure the accuracy and freshness of food\n",
            "\n",
            "To prevent these problems from recurring, the company can:\n",
            "\n",
            "1. Implement real-time inventory tracking and automatic reordering of popular items\n",
            "2. Streamline food delivery/pick-up protocols through technology and better communication with delivery services\n",
            "3. Regularly monitor and enforce food quality standards, addressing any issues quickly and effectively to maintain customer trust and satisfaction.\n",
            "\n",
            "\n",
            "ğŸ”¹ **Topic 3:** Topic related to: back, place, time, go, would\n",
            "ğŸ’¡ **AI Suggestion:**  1. Problems:\n",
            "    * Long waiting times (back) in customer service queues\n",
            "    * Inconvenient service locations (place)\n",
            "    * Inconsistent service quality (time) across different outlets or staff\n",
            "\n",
            "2. Changes:\n",
            "    * Improve customer service response time\n",
            "    * Expand or adjust service locations for better accessibility\n",
            "    * Implement strict quality control measures and regular staff training\n",
            "\n",
            "3. Prevention:\n",
            "    * Invest in technology to streamline customer service processes and reduce wait times\n",
            "    * Regularly survey customer feedback to identify and address location concerns\n",
            "    * Monitor service quality through KPIs and maintain constant communication with employees to ensure standards are met.\n",
            "\n",
            "\n",
            "ğŸ”¹ **Topic 4:** Topic related to: food, good, ever, way, never\n",
            "ğŸ’¡ **AI Suggestion:**  ğŸ“Œ **Top 3 Problems Customers Face:**\n",
            "\n",
            "1. **Inconsistent Quality:** Customers often complain about the variability in the quality of food served, with some meals not meeting their expectations.\n",
            "2. **Long Wait Times:** Another common complaint is long wait times for seating or service, leading to customer frustration and dissatisfaction.\n",
            "3. **Poor Staff Behavior:** Customers have also reported instances of poor behavior from staff, including rudeness, inattentiveness, and lack of knowledge about the menu.\n",
            "\n",
            "ğŸ“Œ **Specific Changes to Make:**\n",
            "\n",
            "1. **Standardize Food Preparation:** Implement strict guidelines for food preparation and ingredients to ensure consistent quality.\n",
            "2. **Improve Service Efficiency:** Train staff to be more efficient in their duties, reducing wait times and increasing the number of customers served in a given time frame.\n",
            "3. **Enhance Staff Training:** Provide comprehensive training programs that emphasize professionalism, product knowledge, and customer service skills.\n",
            "\n",
            "ğŸ“Œ **Preventing Issues Permanently:**\n",
            "\n",
            "1. **Regular Inspections & Audits:** Conduct regular quality checks on food products and service procedures to maintain high standards and identify areas for improvement.\n",
            "2. **Feedback Collection & Analysis:** Regularly collect customer feedback and analyze trends to proactively address potential issues before they become widespread problems.\n",
            "3. **Incentivize Staff Performance:** Motivate staff through rewards and recognition for exceptional performance, fostering a positive work environment and encouraging continuous improvement in service and food quality.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "# !pip install gensim\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# Install dependencies (if running in a new environment)\n",
        "# !pip install gensim nltk\n",
        "\n",
        "# Download necessary NLP resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# OpenRouter API Key\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "\n",
        "# Function to generate AI response using OpenRouter API\n",
        "def generate_ai_response(prompt):\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "        data=json.dumps({\n",
        "            \"model\": \"mistralai/mixtral-8x7b-instruct\",  # Choose a powerful OpenRouter model\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        })\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except KeyError:\n",
        "        return \"Error: Failed to retrieve AI response.\"\n",
        "\n",
        "# Load negative sentiment reviews\n",
        "file_path = '/content/yelp_labelled.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "negative_reviews = [line.split('\\t')[0] for line in lines if line.strip().endswith('0')]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Preprocess negative feedback\n",
        "processed_feedback = [preprocess(text) for text in negative_reviews if text.strip()]\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(processed_feedback)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_feedback]\n",
        "\n",
        "# # Train LDA model\n",
        "# num_topics = 5\n",
        "# lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "# # Topic descriptions\n",
        "# topic_descriptions = {\n",
        "#     0: \"Poor product quality and usability issues.\",\n",
        "#     1: \"Dissatisfaction with pricing and value for money.\",\n",
        "#     2: \"Complaints about battery life and charging problems.\",\n",
        "#     3: \"Issues with customer service and post-purchase support.\",\n",
        "#     4: \"Concerns over product durability and long-term performance.\"\n",
        "# }\n",
        "# import gensim\n",
        "# import gensim.corpora as corpora\n",
        "\n",
        "# Assume 'corpus' and 'dictionary' are already prepared\n",
        "num_topics = 5\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "# Dynamically generate topic descriptions\n",
        "topic_descriptions = {}\n",
        "\n",
        "for i, topic in lda_model.show_topics(num_topics=num_topics, num_words=5, formatted=False):\n",
        "    top_words = \", \".join([word for word, _ in topic])  # Extract top words\n",
        "    topic_descriptions[i] = f\"Topic related to: {top_words}\"\n",
        "\n",
        "# Print generated topic descriptions\n",
        "for topic_id, description in topic_descriptions.items():\n",
        "    print(f\"Topic {topic_id}: {description}\")\n",
        "\n",
        "print(\"\\nExtracted Topics and AI Suggestions:\\n\")\n",
        "\n",
        "for topic_id, topic_text in topic_descriptions.items():\n",
        "    print(f\"\\nğŸ”¹ **Topic {topic_id}:** {topic_text}\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    **Customer Complaints:** {topic_text}\n",
        "\n",
        "    ğŸ“Œ **What are the top 3 problems customers face?**\n",
        "    ğŸ“Œ **What specific changes should the company make?**\n",
        "    ğŸ“Œ **How can the company prevent these issues permanently?**\n",
        "    Keep responses **short and structured**.\n",
        "    \"\"\"\n",
        "\n",
        "    ai_suggestion = generate_ai_response(prompt)\n",
        "    print(f\"ğŸ’¡ **AI Suggestion:** {ai_suggestion}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy gensim pyLDAvis numba tensorflow tensorflow-text tf-keras cudf-cu12 cuml-cu12 dask-cuda distributed-ucxx-cu12 raft-dask-cu12 dask-cudf-cu12\n",
        "!pip install numpy==1.26.4 numba==0.60.0 tensorflow==2.18.0 tensorflow-text==2.18.1 tf-keras==2.18.0\n",
        "!pip install cudf-cu12==25.2 cuml-cu12==25.2 dask-cuda==25.2 distributed-ucxx-cu12==0.42 raft-dask-cu12==25.2 dask-cudf-cu12==25.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6a-QwdtcKl1",
        "outputId": "5c667a9c-66fb-4efc-dc9c-6cbc98ef79c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pyLDAvis as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: tensorflow-text 2.18.1\n",
            "Uninstalling tensorflow-text-2.18.1:\n",
            "  Successfully uninstalled tensorflow-text-2.18.1\n",
            "Found existing installation: tf_keras 2.18.0\n",
            "Uninstalling tf_keras-2.18.0:\n",
            "  Successfully uninstalled tf_keras-2.18.0\n",
            "Found existing installation: cudf-cu12 25.2.1\n",
            "Uninstalling cudf-cu12-25.2.1:\n",
            "  Successfully uninstalled cudf-cu12-25.2.1\n",
            "Found existing installation: cuml-cu12 25.2.1\n",
            "Uninstalling cuml-cu12-25.2.1:\n",
            "  Successfully uninstalled cuml-cu12-25.2.1\n",
            "Found existing installation: dask-cuda 25.2.0\n",
            "Uninstalling dask-cuda-25.2.0:\n",
            "  Successfully uninstalled dask-cuda-25.2.0\n",
            "Found existing installation: distributed-ucxx-cu12 0.42.0\n",
            "Uninstalling distributed-ucxx-cu12-0.42.0:\n",
            "  Successfully uninstalled distributed-ucxx-cu12-0.42.0\n",
            "Found existing installation: raft-dask-cu12 25.2.0\n",
            "Uninstalling raft-dask-cu12-25.2.0:\n",
            "  Successfully uninstalled raft-dask-cu12-25.2.0\n",
            "Found existing installation: dask-cudf-cu12 25.2.2\n",
            "Uninstalling dask-cudf-cu12-25.2.2:\n",
            "  Successfully uninstalled dask-cudf-cu12-25.2.2\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numba==0.60.0\n",
            "  Downloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting tensorflow==2.18.0\n",
            "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-text==2.18.1\n",
            "  Downloading tensorflow_text-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tf-keras==2.18.0\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.60.0) (0.43.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, numba, tensorflow, tf-keras, tensorflow-text\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numba-0.60.0 numpy-1.26.4 tensorflow-2.18.0 tensorflow-text-2.18.1 tf-keras-2.18.0\n",
            "Collecting cudf-cu12==25.2\n",
            "  Downloading cudf_cu12-25.2.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting cuml-cu12==25.2\n",
            "  Downloading cuml_cu12-25.2.0.tar.gz (2.5 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dask-cuda==25.2\n",
            "  Downloading dask_cuda-25.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting distributed-ucxx-cu12==0.42\n",
            "  Downloading distributed_ucxx_cu12-0.42.0.tar.gz (997 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting raft-dask-cu12==25.2\n",
            "  Downloading raft_dask_cu12-25.2.0.tar.gz (5.6 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dask-cudf-cu12==25.2\n",
            "  Downloading dask_cudf_cu12-25.2.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (5.5.2)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.6.2 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (12.6.2.post1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (13.3.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (2025.3.2)\n",
            "Requirement already satisfied: libcudf-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (25.2.1)\n",
            "Requirement already satisfied: numba-cuda<0.3.0a0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (0.2.0)\n",
            "Requirement already satisfied: numba<0.61.0a0,>=0.59.1 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (0.60.0)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (1.26.4)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (0.2.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (24.2)\n",
            "Requirement already satisfied: pandas<2.2.4dev0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20.0.0a0,>=14.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (18.1.0)\n",
            "Requirement already satisfied: pylibcudf-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (25.2.1)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (0.5.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (13.9.4)\n",
            "Requirement already satisfied: rmm-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (25.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12==25.2) (4.13.2)\n",
            "Requirement already satisfied: cuvs-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (25.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (1.4.2)\n",
            "Requirement already satisfied: libcuml-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (25.2.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cufft-cu12 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (11.2.3.61)\n",
            "Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (10.3.6.82)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (12.5.1.3)\n",
            "Requirement already satisfied: pylibraft-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (25.2.0)\n",
            "Requirement already satisfied: rapids-dask-dependency==25.2.* in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (25.2.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (1.15.2)\n",
            "Requirement already satisfied: treelite==4.4.1 in /usr/local/lib/python3.11/dist-packages (from cuml-cu12==25.2) (4.4.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask-cuda==25.2) (8.1.8)\n",
            "Requirement already satisfied: pynvml<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from dask-cuda==25.2) (12.0.0)\n",
            "Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dask-cuda==25.2) (3.0.0)\n",
            "Requirement already satisfied: ucxx-cu12==0.42.* in /usr/local/lib/python3.11/dist-packages (from distributed-ucxx-cu12==0.42) (0.42.0)\n",
            "Requirement already satisfied: libraft-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from raft-dask-cu12==25.2) (25.2.0)\n",
            "Requirement already satisfied: ucx-py-cu12==0.42.* in /usr/local/lib/python3.11/dist-packages (from raft-dask-cu12==25.2) (0.42.0)\n",
            "Requirement already satisfied: libcuvs-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from cuvs-cu12==25.2.*->cuml-cu12==25.2) (25.2.1)\n",
            "Requirement already satisfied: libkvikio-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from libcudf-cu12==25.2.*->cudf-cu12==25.2) (25.2.1)\n",
            "Requirement already satisfied: nvidia-nvcomp-cu12==4.2.0.11 in /usr/local/lib/python3.11/dist-packages (from libcudf-cu12==25.2.*->cudf-cu12==25.2) (4.2.0.11)\n",
            "Requirement already satisfied: dask==2024.12.1 in /usr/local/lib/python3.11/dist-packages (from rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (2024.12.1)\n",
            "Requirement already satisfied: distributed==2024.12.1 in /usr/local/lib/python3.11/dist-packages (from rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (2024.12.1)\n",
            "Requirement already satisfied: dask-expr==1.1.21 in /usr/local/lib/python3.11/dist-packages (from rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (1.1.21)\n",
            "Requirement already satisfied: libucx-cu12<1.19,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from ucx-py-cu12==0.42.*->raft-dask-cu12==25.2) (1.18.1)\n",
            "Requirement already satisfied: libucxx-cu12==0.42.* in /usr/local/lib/python3.11/dist-packages (from ucxx-cu12==0.42.*->distributed-ucxx-cu12==0.42) (0.42.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (3.1.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (8.7.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (3.1.6)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (3.1.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (6.4.2)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (2.4.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==25.2) (0.8.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<0.61.0a0,>=0.59.1->cudf-cu12==25.2) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2) (2025.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml<13.0.0a0,>=12.0.0->dask-cuda==25.2) (12.570.86)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cufft-cu12->cuml-cu12==25.2) (12.5.82)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->cudf-cu12==25.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->cudf-cu12==25.2) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12==25.2) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.4dev0,>=2.0->cudf-cu12==25.2) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.10.3->distributed==2024.12.1->rapids-dask-dependency==25.2.*->cuml-cu12==25.2) (3.0.2)\n",
            "Downloading cudf_cu12-25.2.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask_cuda-25.2.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask_cudf_cu12-25.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cuml-cu12, distributed-ucxx-cu12, raft-dask-cu12\n",
            "  Building wheel for cuml-cu12 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cuml-cu12: filename=cuml_cu12-25.2.0-cp311-cp311-manylinux_2_28_x86_64.whl size=9744534 sha256=f7085b1691cacc13fb7d7ef8ee9d69a68fd1c8cd33100a669f99822095244949\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/7f/40/47191807423e4a660a1ff0ff9b89716c558cdc8543cacc3248\n",
            "  Building wheel for distributed-ucxx-cu12 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distributed-ucxx-cu12: filename=distributed_ucxx_cu12-0.42.0-py3-none-any.whl size=24814 sha256=24b4a9541965904448582b805c1b66c8f3d5a319fb86d73b2656877cc3a15694\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/6f/c8/218f0214f308d7f7308daffa946c2ae614a09478732659bd74\n",
            "  Building wheel for raft-dask-cu12 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for raft-dask-cu12: filename=raft_dask_cu12-25.2.0-cp311-cp311-manylinux_2_28_x86_64.whl size=293515763 sha256=e8180027a9a0d2830c555d54398059a04294b5e655aa40b88c416eec4a3a908b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/3a/ec/7def1e0bb02ac32c18f0f8234e4c692f73ea9c12744129969f\n",
            "Successfully built cuml-cu12 distributed-ucxx-cu12 raft-dask-cu12\n",
            "Installing collected packages: cudf-cu12, distributed-ucxx-cu12, dask-cudf-cu12, dask-cuda, raft-dask-cu12, cuml-cu12\n",
            "Successfully installed cudf-cu12-25.2.0 cuml-cu12-25.2.0 dask-cuda-25.2.0 dask-cudf-cu12-25.2.0 distributed-ucxx-cu12-0.42.0 raft-dask-cu12-25.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import os\n",
        "\n",
        "# Ensure required NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# OpenRouter API Key\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "def generate_ai_response(prompt):\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "        data=json.dumps({\n",
        "            \"model\": \"mistralai/mixtral-8x7b-instruct\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        })\n",
        "    )\n",
        "    try:\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except KeyError:\n",
        "        return \"Error: Failed to retrieve AI response.\"\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "def analyze_trends(df):\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    ax = sns.countplot(x=df['Sentiment'], palette=['red', 'green'])\n",
        "    ax.set_xticklabels([\"Negative Reviews\", \"Positive Reviews\"])\n",
        "    plt.title(\"Review Sentiment Distribution\")\n",
        "    plt.savefig(\"sentiment_analysis.png\")\n",
        "    plt.close()\n",
        "    return \"Sentiment trend analysis completed. See the plot below.\", \"sentiment_analysis.png\"\n",
        "\n",
        "def process_input(file, text):\n",
        "    df = None  # Initialize dataframe variable\n",
        "\n",
        "    if file is not None:\n",
        "        file_path =file  # Adjust path for Gradio\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        # Handle CSV File\n",
        "        if file_extension == \".csv\":\n",
        "            df = pd.read_csv(file_path, encoding=\"utf-8\", nrows=5000,on_bad_lines=\"skip\")  # Load first 5000 rows\n",
        "\n",
        "        # Handle Text File\n",
        "        elif file_extension == \".txt\":\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            # Parse text file (split by tab into 'Review' and 'Sentiment')\n",
        "            data = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
        "            df = pd.DataFrame(data, columns=[\"Review\", \"Sentiment\"])\n",
        "            df.columns = [\"Review\", \"Sentiment\"]\n",
        "            df[\"Sentiment\"] = df[\"Sentiment\"].astype(int)  # Convert sentiment to int\n",
        "\n",
        "        else:\n",
        "            return \"Unsupported file format. Please upload a CSV or TXT file.\"\n",
        "\n",
        "    elif text:\n",
        "        df = pd.DataFrame([[text, 0]], columns=[\"Review\", \"Sentiment\"])\n",
        "\n",
        "    else:\n",
        "        return \"Please provide either a file or a text review.\"\n",
        "\n",
        "    # Preprocess reviews\n",
        "    df[\"Processed_Review\"] = df[\"Review\"].astype(str).apply(preprocess)\n",
        "\n",
        "    # ğŸ›‘ **Filter Only Negative Reviews (`Sentiment == 0`)**\n",
        "    df_negative = df[df[\"Sentiment\"] == 0]\n",
        "\n",
        "    if df_negative.empty:\n",
        "        return \"No negative reviews found.\", \"Sentiment trend analysis completed.\", \"sentiment_analysis.png\"\n",
        "\n",
        "    # Topic Modeling on Negative Reviews\n",
        "    dictionary = corpora.Dictionary(df_negative[\"Processed_Review\"])\n",
        "    corpus = [dictionary.doc2bow(review) for review in df_negative[\"Processed_Review\"]]\n",
        "    num_topics = 5\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "    topic_descriptions = {i: \", \".join([word for word, _ in topic]) for i, topic in lda_model.show_topics(num_topics=num_topics, num_words=5, formatted=False)}\n",
        "\n",
        "    # Sentiment Analysis Plot\n",
        "    sentiment_analysis, plot_path = analyze_trends(df)\n",
        "\n",
        "    # Generate AI Suggestions **Only for Negative Topics**\n",
        "    results = []\n",
        "    for topic_id, topic_text in topic_descriptions.items():\n",
        "        prompt = f\"Customer Complaints: {topic_text}\\n\\nTop 3 problems customers face?\\nWhat changes should the company make?\\nHow to prevent these issues permanently?\"\n",
        "        ai_suggestion = generate_ai_response(prompt)\n",
        "        results.append(f\"Topic {topic_id}: {topic_text}\\nAI Suggestion: {ai_suggestion}\\n\")\n",
        "\n",
        "    output_text = \"\\n\".join(results)\n",
        "\n",
        "    return output_text, sentiment_analysis, plot_path\n",
        "\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "body {\n",
        "    background-color: #fdf6ff;\n",
        "    font-family: 'Comic Sans MS', cursive, sans-serif;\n",
        "}\n",
        "h2 {\n",
        "    color: #7d5ba6;\n",
        "    font-weight: bold;\n",
        "}\n",
        ".gr-button-primary {\n",
        "    background: linear-gradient(135deg, #ffb6b9, #fae3d9);\n",
        "    color: #333;\n",
        "    border: none;\n",
        "    font-weight: bold;\n",
        "    border-radius: 8px;\n",
        "}\n",
        ".gr-button-primary:hover {\n",
        "    background: linear-gradient(135deg, #fae3d9, #ffb6b9);\n",
        "}\n",
        ".container {\n",
        "    padding: 25px;\n",
        "    border-radius: 15px;\n",
        "    background: #ffffff;\n",
        "    box-shadow: 0 8px 20px rgba(0,0,0,0.1);\n",
        "}\n",
        "label {\n",
        "    font-weight: bold;\n",
        "    color: #5d3a9b;\n",
        "}\n",
        "textarea, input {\n",
        "    border-radius: 10px !important;\n",
        "    border: 1px solid #e0c3fc !important;\n",
        "    padding: 10px !important;\n",
        "    background: #fff5f9 !important;\n",
        "}\n",
        "\"\"\") as demo:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"<h2 style='text-align:center;'>âœ¨ Customer Review Analysis âœ¨</h2>\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"ğŸ“‚ Upload a CSV/TXT file with 'Review' and 'Sentiment'\", type=\"filepath\")\n",
        "            text_input = gr.Textbox(label=\"ğŸ’¬ Or manually enter a review:\", placeholder=\"E.g., The product quality was disappointing...\", lines=3)\n",
        "            submit_button = gr.Button(\"ğŸ€ Generate Suggestions\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            output = gr.Textbox(label=\"ğŸŒ¸ AI Suggestions (Based on Topics)\", interactive=False, lines=10)\n",
        "            sentiment_output = gr.Textbox(label=\"ğŸ“Š Sentiment Analysis Summary\", interactive=False)\n",
        "            trend_plot = gr.Image(label=\"ğŸ“ˆ Sentiment Distribution Plot\")\n",
        "\n",
        "    submit_button.click(process_input, inputs=[file_input, text_input], outputs=[output, sentiment_output, trend_plot])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "aYnf1tdqdVrA",
        "outputId": "193941a1-bd6d-4714-c9b6-bf8c0676fff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1a1434875e3e48e5a1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1a1434875e3e48e5a1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPmYLAU_AY6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXSxViR43Dm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install pandas\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAP9y5J-fDyq",
        "outputId": "ca5615de-cc81-43fb-c4a9-a9cfedd9d0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "Successfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import os\n",
        "\n",
        "# Ensure required NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# OpenRouter API Key\n",
        "OPENROUTER_API_KEY = \"\"\n",
        "\n",
        "def generate_ai_response(prompt):\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "        data=json.dumps({\n",
        "            \"model\": \"mistralai/mixtral-8x7b-instruct\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        })\n",
        "    )\n",
        "    try:\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except KeyError:\n",
        "        return \"Error: Failed to retrieve AI response.\"\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "def analyze_trends(df):\n",
        "    sentiment_counts = df['Sentiment'].value_counts().sort_index()\n",
        "    total_reviews = sentiment_counts.sum()\n",
        "    negative = sentiment_counts.get(0, 0)\n",
        "    positive = sentiment_counts.get(1, 0)\n",
        "    negative_pct = (negative / total_reviews) * 100 if total_reviews > 0 else 0\n",
        "    positive_pct = (positive / total_reviews) * 100 if total_reviews > 0 else 0\n",
        "\n",
        "    summary = (\n",
        "        f\"Total Reviews: {total_reviews}\\n\"\n",
        "        f\"Positive Reviews: {positive} ({positive_pct:.2f}%)\\n\"\n",
        "        f\"Negative Reviews: {negative} ({negative_pct:.2f}%)\"\n",
        "    )\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    ax = sns.countplot(x=df['Sentiment'], palette=['red', 'green'])\n",
        "    ax.set_xticklabels([\"Negative Reviews\", \"Positive Reviews\"])\n",
        "    plt.title(\"Review Sentiment Distribution\")\n",
        "    plt.savefig(\"sentiment_analysis.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return summary, \"sentiment_analysis.png\"\n",
        "\n",
        "\n",
        "def process_input(file, text, custom_prompt):\n",
        "    df = None  # Initialize dataframe variable\n",
        "\n",
        "    if file is not None:\n",
        "        file_path = file  # Adjust path for Gradio\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        # Handle CSV File\n",
        "        if file_extension == \".csv\":\n",
        "            df = pd.read_csv(file_path, encoding=\"utf-8\", nrows=5000, on_bad_lines=\"skip\")  # Load first 5000 rows\n",
        "\n",
        "        # Handle Text File\n",
        "        elif file_extension == \".txt\":\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "            data = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]\n",
        "            df = pd.DataFrame(data, columns=[\"Review\", \"Sentiment\"])\n",
        "            df[\"Sentiment\"] = df[\"Sentiment\"].astype(int)\n",
        "\n",
        "        else:\n",
        "            return \"Unsupported file format. Please upload a CSV or TXT file.\", \"\", None\n",
        "\n",
        "    elif text:\n",
        "        df = pd.DataFrame([[text, 0]], columns=[\"Review\", \"Sentiment\"])\n",
        "\n",
        "    else:\n",
        "        return \"Please provide either a file or a text review.\", \"\", None\n",
        "\n",
        "    df[\"Processed_Review\"] = df[\"Review\"].astype(str).apply(preprocess)\n",
        "\n",
        "    df_negative = df[df[\"Sentiment\"] == 0]\n",
        "\n",
        "    if df_negative.empty:\n",
        "        return \"No negative reviews found.\", \"Sentiment trend analysis completed.\", \"sentiment_analysis.png\"\n",
        "\n",
        "    dictionary = corpora.Dictionary(df_negative[\"Processed_Review\"])\n",
        "    corpus = [dictionary.doc2bow(review) for review in df_negative[\"Processed_Review\"]]\n",
        "    num_topics = 5\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "    topic_descriptions = {\n",
        "        i: \", \".join([word for word, _ in topic])\n",
        "        for i, topic in lda_model.show_topics(num_topics=num_topics, num_words=5, formatted=False)\n",
        "    }\n",
        "\n",
        "    sentiment_analysis, plot_path = analyze_trends(df)\n",
        "\n",
        "    results = []\n",
        "    for topic_id, topic_text in topic_descriptions.items():\n",
        "        prompt = f\"Topic: {topic_text}\\n{custom_prompt}\"\n",
        "        ai_suggestion = generate_ai_response(prompt)\n",
        "        results.append(f\"Topic {topic_id}: {topic_text}\\nAI Suggestion: {ai_suggestion}\\n\")\n",
        "\n",
        "    output_text = \"\\n\".join(results)\n",
        "    return output_text, sentiment_analysis, plot_path\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "body {\n",
        "    background: linear-gradient(135deg, #f7a1d7, #f0c7fa);\n",
        "    font-family: 'Comic Sans MS', cursive, sans-serif;\n",
        "    background-attachment: fixed;\n",
        "    background-size: cover;\n",
        "}\n",
        "h2 {\n",
        "    color: #7d5ba6;\n",
        "    font-weight: bold;\n",
        "    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);\n",
        "}\n",
        ".gr-button-primary {\n",
        "    background: linear-gradient(135deg, #ffb6b9, #fae3d9);\n",
        "    color: #333;\n",
        "    border: none;\n",
        "    font-weight: bold;\n",
        "    border-radius: 8px;\n",
        "    box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
        "}\n",
        ".gr-button-primary:hover {\n",
        "    background: linear-gradient(135deg, #fae3d9, #ffb6b9);\n",
        "    box-shadow: 0 4px 8px rgba(0,0,0,0.2);\n",
        "}\n",
        ".container {\n",
        "    padding: 25px;\n",
        "    border-radius: 15px;\n",
        "    background: rgba(255, 255, 255, 0.9);  /* Semi-transparent white background */\n",
        "    box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "label {\n",
        "    font-weight: bold;\n",
        "    color: #5d3a9b;\n",
        "}\n",
        "textarea, input {\n",
        "    border-radius: 10px !important;\n",
        "    border: 1px solid #e0c3fc !important;\n",
        "    padding: 10px !important;\n",
        "    background: #fff5f9 !important;\n",
        "    transition: background-color 0.3s ease;\n",
        "}\n",
        "textarea:focus, input:focus {\n",
        "    background: #f4d0e1 !important;\n",
        "    border-color: #d3a0d0 !important;\n",
        "}\n",
        "\"\"\") as demo:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"<h2 style='text-align:center;'>âœ¨ Customer Review Analysis âœ¨</h2>\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"ğŸ“‚ Upload a CSV/TXT file with 'Review' and 'Sentiment'\", type=\"filepath\")\n",
        "            text_input = gr.Textbox(label=\"ğŸ’¬ Or manually enter a review:\", placeholder=\"E.g., The product quality was disappointing...\", lines=3)\n",
        "\n",
        "            prompt_input = gr.Textbox(\n",
        "                label=\"ğŸ”® Enter a custom prompt for the AI:\",\n",
        "                placeholder=\"E.g., What are the top problems customers are facing?\",\n",
        "                lines=3\n",
        "            )\n",
        "\n",
        "            submit_button = gr.Button(\"ğŸ€ Generate Suggestions\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            output = gr.Textbox(label=\"ğŸŒ¸ AI Suggestions (Based on Topics)\", interactive=False, lines=10)\n",
        "            sentiment_output = gr.Textbox(label=\"ğŸ“Š Sentiment Analysis Summary\", interactive=False)\n",
        "            trend_plot = gr.Image(label=\"ğŸ“ˆ Sentiment Distribution Plot\")\n",
        "\n",
        "    submit_button.click(process_input, inputs=[file_input, text_input, prompt_input], outputs=[output, sentiment_output, trend_plot])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "qP9M0cDJAaJh",
        "outputId": "72b33bda-8704-42f1-a8d6-984855760f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://92000b597f6ed4dcef.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://92000b597f6ed4dcef.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}